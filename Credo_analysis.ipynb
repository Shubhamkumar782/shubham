{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    " \n",
    "\n",
    "data_frame = pd.read_excel(\"(Jackie Global) Global Services Comments.xlsx\")\n",
    "data_frame = data_frame.rename(columns={'If there is one thing that would improve your [Performance, Speed of Decision-making, Innovation ] what would it be (English)?': 'Comment'})\n",
    "data_frame.head()\n",
    "\n",
    " \n",
    "\n",
    "Innovation_df = data_frame[data_frame[\"Question\"] == \"Innovation\"]\n",
    "Innovation_df.head()\n",
    "\n",
    " \n",
    "\n",
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    " \n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "Innovation_df['Comment_clean'] = Innovation_df['Comment'].apply(lambda x: finalpreprocess(x))\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Innovation_df_clean=Innovation_df.iloc[:,3:]\n",
    "Innovation_df_clean.head()\n",
    "\n",
    " \n",
    "\n",
    "# Word2Vec runs on tokenized sentences\n",
    "Innovation_df_token= [nltk.word_tokenize(i) for i in Innovation_df_clean[\"Comment_clean\"]]\n",
    "\n",
    " \n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer:\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    " \n",
    "\n",
    "Innovation_df_clean['Innovation_df_tok']=[nltk.word_tokenize(i) for i in Innovation_df_clean[\"Comment_clean\"]] \n",
    "model = Word2Vec(Innovation_df_clean['Innovation_df_tok'],min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key , model.wv.vectors))\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "# converting text to numerical data using Word2Vec\n",
    "Innovation_df_token_w2v = modelw.transform(Innovation_df_token)\n",
    "\n",
    " \n",
    "\n",
    "# Kmean_model_word2vec = KMeans(n_clusters=10, init='k-means++', max_iter=200, n_init=15,random_state=20)\n",
    "# Kmean_model_word2vec.fit(performance_df_token_w2v)\n",
    "\n",
    " \n",
    "\n",
    "import pickle\n",
    "\n",
    " \n",
    "\n",
    "with open(\"Kmeans_final_word2vec_model.pickle\", \"rb\") as f:\n",
    "    Kmean_model_word2vec = pickle.load(f)\n",
    "    \n",
    "Kmean_word2vec_clus= Kmean_model_word2vec.fit_predict(Innovation_df_token_w2v)\n",
    "\n",
    " \n",
    "\n",
    "Innovation_df_clean[\"cluster\"]= Kmean_word2vec_clus.tolist()\n",
    "\n",
    " \n",
    "\n",
    "Innovation_df_clean[\"cluster\"].value_counts()\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Innovation_df_clean[[\"Comment_clean\",\"cluster\"]].to_csv(\"ae.csv\")\n",
    "\n",
    " \n",
    "\n",
    "## Agglomerative clustering\n",
    "\n",
    " \n",
    "\n",
    "import pickle\n",
    "\n",
    " \n",
    "\n",
    "with open(\"Agg_final_word2vec_model.pkl\", \"rb\") as f:\n",
    "    Agg_model_word2vec = pickle.load(f)\n",
    "    \n",
    "Kmean_word2vec_clus= Agg_model_word2vec.fit_predict(Innovation_df_token_w2v)\n",
    "\n",
    " \n",
    "\n",
    "Innovation_df_clean[\"cluster_agg\"]= Kmean_word2vec_clus.tolist()\n",
    "\n",
    " \n",
    "\n",
    "Innovation_df_clean[\"cluster_agg\"].value_counts()\n",
    "\n",
    " \n",
    "\n",
    "Decision_df_clean[[\"Comment_clean\",\"cluster_agg\"]].to_csv(\"af.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
