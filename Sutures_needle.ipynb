{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path,PurePath\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Path.cwd()\n",
    "path = Path(r'D:\\Users\\GPatil13\\Downloads')\n",
    "os.chdir(path)\n",
    "os.getcwd()\n",
    "os.listdir()\n",
    "\n",
    " \n",
    "\n",
    "main_df = pd.read_excel('Area_Sutures_out - Copy.xlsx')\n",
    "df =main_df.sample(frac =0.16)\n",
    "df.shape\n",
    "\n",
    " \n",
    "\n",
    "df.head()\n",
    "\n",
    " \n",
    "\n",
    "use_cols = ['Description','USP', 'Length', 'Suture Color', 'Needle Name','Brandname']\n",
    "df = df[use_cols]\n",
    "df\n",
    "\n",
    " \n",
    "\n",
    "df['USP'] = df['USP'].str.replace('-0','')\n",
    "\n",
    " \n",
    "\n",
    "df['USP'].value_counts()#.count()\n",
    "\n",
    " \n",
    "\n",
    "df['Length'].value_counts().count(), df['Brandname'].value_counts().count()\n",
    "\n",
    " \n",
    "\n",
    "# for col in df.iloc[:,1:].columns:\n",
    "#     print(df[col].value_counts())\n",
    "\n",
    " \n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase text\n",
    "    text=text.strip() #get rid of leading/trailing whitespace\n",
    "    text=re.compile('|<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.sub('\\s+', ' ', text) #Remove extra space and tabs\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace\n",
    "\n",
    " \n",
    "\n",
    "    return text\n",
    "\n",
    " \n",
    "\n",
    "df['Description'] = df['Description'].apply(lambda x: preprocess(x))\n",
    "\n",
    " \n",
    "\n",
    "df.head()#['Description']#df_usp =  \n",
    "\n",
    " \n",
    "\n",
    "### Check Nan values\n",
    "\n",
    " \n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    " \n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    " \n",
    "\n",
    "### lets use tf-idf now\n",
    "\n",
    " \n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# for bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    " \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    " \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    " \n",
    "\n",
    "### Lets build the model for USP TV\n",
    "\n",
    " \n",
    "\n",
    "df1[ df1['Description']=='vicryl']#.count()\n",
    "\n",
    " \n",
    "\n",
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split( df['Description'], df['USP'],test_size=0.3,shuffle=True, random_state=1)\n",
    "X_train\n",
    "# As Word2vec runs on Tocckenized data so need to convert into tockens\n",
    "# X_train_tok = [nltk.word_tokenize(i) for i in X_train]\n",
    "# X_test_tok =  [nltk.word_tokenize(i) for i in X_test]\n",
    "\n",
    " \n",
    "\n",
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer( ngram_range=(1,2),use_idf=True)\n",
    "\n",
    " \n",
    "\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_train_vectors_tfidf\n",
    "\n",
    " \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2',random_state=1)\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    " \n",
    "\n",
    "## Lets predict y value for the test dataset now\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "y_predict[:12], y_prob[:12]\n",
    "\n",
    " \n",
    "\n",
    "f1_score = f1_score(y_test, y_predict, average='weighted')\n",
    "f1_score\n",
    "\n",
    " \n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    " \n",
    "\n",
    "print('Confusion Matrix:\\n',confusion_matrix(y_test, y_predict))\n",
    "\n",
    " \n",
    "\n",
    "#### Since ROC-AUC score is onlyapplicable for Binary class classification hence wecant use it here. \n",
    "\n",
    " \n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)\n",
    "\n",
    "### ## Lets Try SVM\n",
    "\n",
    " \n",
    "\n",
    "## Lets Try SVM\n",
    "from sklearn import svm   #, datasets\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    " \n",
    "\n",
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1,random_state=1).fit(X_train_vectors_tfidf, y_train)\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=2,random_state=1).fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    " \n",
    "\n",
    "To calculate the efficiency of the two models, we’ll test the two classifiers using the test data set:\n",
    "\n",
    " \n",
    "\n",
    "## Lets predict y value for the test dataset now\n",
    "poly_pred = poly.predict(X_test_vectors_tfidf)\n",
    "#poly_proba = poly.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "rbf_pred = rbf.predict(X_test_vectors_tfidf)\n",
    "#rbf_proba = rbf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "Finally, we’ll calculate the accuracy and f1 scores for SVM with Polynomial kernel:\n",
    "\n",
    " \n",
    "\n",
    "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
    "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
    "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
    "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n",
    "\n",
    " \n",
    "\n",
    "print(classification_report(y_test, poly_pred))\n",
    "\n",
    " \n",
    "\n",
    "In the same way, the accuracy and f1 scores for SVM with RBF kernel:\n",
    "\n",
    " \n",
    "\n",
    "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
    "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')\n",
    "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
    "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))\n",
    "\n",
    " \n",
    "\n",
    "### Now lets use another TV = 'Length'\n",
    "\n",
    " \n",
    "\n",
    "df.head()\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test =train_test_split(df['Description'],df['Length'],test_size=0.3,shuffle=True,random_state=1)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "X_train\n",
    "\n",
    " \n",
    "\n",
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer( ngram_range=(1,2),use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_train_vectors_tfidf\n",
    "\n",
    " \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2',random_state=1)\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    " \n",
    "\n",
    "## Lets predict y value for the test dataset now\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "y_predict[:12], y_prob[:12]\n",
    "\n",
    " \n",
    "\n",
    "print(accuracy_score(y_test, y_predict))\n",
    "print(f1_score(y_test, y_predict, average='weighted'))\n",
    "\n",
    " \n",
    "\n",
    "### ## Lets Try SVM\n",
    "\n",
    " \n",
    "\n",
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1,random_state=1).fit(X_train_vectors_tfidf, y_train)\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1,random_state=1).fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    " \n",
    "\n",
    "To calculate the efficiency of the two models, we’ll test the two classifiers using the test data set:\n",
    "\n",
    " \n",
    "\n",
    "## Lets predict y value for the test dataset now\n",
    "poly_pred = poly.predict(X_test_vectors_tfidf)\n",
    "#poly_proba = poly.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "rbf_pred = rbf.predict(X_test_vectors_tfidf)\n",
    "#rbf_proba = rbf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "Finally, we’ll calculate the accuracy and f1 scores for SVM with Polynomial kernel:\n",
    "\n",
    " \n",
    "\n",
    "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
    "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
    "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
    "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n",
    "\n",
    " \n",
    "\n",
    "print(classification_report(y_test, poly_pred))\n",
    "\n",
    " \n",
    "\n",
    "In the same way, the accuracy and f1 scores for SVM with RBF kernel:\n",
    "\n",
    " \n",
    "\n",
    "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
    "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')\n",
    "\n",
    " \n",
    "\n",
    "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
    "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))\n",
    "\n",
    " \n",
    "\n",
    "### Now lets build models on 3rd TV = 'Suture Color'\n",
    "\n",
    " \n",
    "\n",
    " df['Suture Color'].value_counts()\n",
    "\n",
    " \n",
    "\n",
    "from sklearn import preprocessing\n",
    " \n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    " \n",
    "# Encode labels in column 'species'.\n",
    "df['Suture Color']= label_encoder.fit_transform(df['Suture Color'])\n",
    " \n",
    "df['Suture Color'].unique()\n",
    "\n",
    " \n",
    "\n",
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Description'],df['Suture Color'],\n",
    "                                                    test_size=0.3,shuffle=True,random_state=1)\n",
    "\n",
    " \n",
    "\n",
    "X_train\n",
    "\n",
    " \n",
    "\n",
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer( ngram_range=(1,2),use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_train_vectors_tfidf\n",
    "\n",
    "#### Try Logistic Regression\n",
    "\n",
    " \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2',random_state=1)\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    " \n",
    "\n",
    "## Lets predict y value for the test dataset now\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "y_predict[:12], y_prob[:12]\n",
    "\n",
    " \n",
    "\n",
    "print(accuracy_score(y_test, y_predict))\n",
    "print(f1_score(y_test, y_predict, average='weighted'))\n",
    "\n",
    " \n",
    "\n",
    "#### Try SVM\n",
    "\n",
    " \n",
    "\n",
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1,random_state=1).fit(X_train_vectors_tfidf, y_train)\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1,random_state=1).fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    " \n",
    "\n",
    "To calculate the efficiency of the two models, we’ll test the two classifiers using the test data set:\n",
    "\n",
    " \n",
    "\n",
    "## Lets predict y value for the test dataset now\n",
    "poly_pred = poly.predict(X_test_vectors_tfidf)\n",
    "#poly_proba = poly.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "rbf_pred = rbf.predict(X_test_vectors_tfidf)\n",
    "#rbf_proba = rbf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "Finally, we’ll calculate the accuracy and f1 scores for SVM with Polynomial kernel:\n",
    "\n",
    " \n",
    "\n",
    "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
    "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
    "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
    "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n",
    "\n",
    " \n",
    "\n",
    "print(classification_report(y_test, poly_pred))\n",
    "\n",
    " \n",
    "\n",
    "In the same way, the accuracy and f1 scores for SVM with RBF kernel:\n",
    "\n",
    " \n",
    "\n",
    "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
    "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')\n",
    "\n",
    " \n",
    "\n",
    "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
    "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))\n",
    "\n",
    " \n",
    "\n",
    "### Lets try now Last TV = 'Brandname'\n",
    "\n",
    " \n",
    "\n",
    "df.head()\n",
    "\n",
    " \n",
    "\n",
    "from sklearn import preprocessing\n",
    " \n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    " \n",
    "# Encode labels in column 'species'.\n",
    "df['Brandname']= label_encoder.fit_transform(df['Brandname'])\n",
    " \n",
    "df['Brandname'].unique()\n",
    "\n",
    " \n",
    "\n",
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split( df['Description'],df['Brandname'],test_size=0.3,shuffle=True,random_state=1)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "X_train\n",
    "\n",
    " \n",
    "\n",
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer( ngram_range=(1,2),use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_train_vectors_tfidf\n",
    "\n",
    " \n",
    "\n",
    "#### Try Logistic Regression\n",
    "\n",
    " \n",
    "\n",
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2',random_state=1)\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    " \n",
    "\n",
    "## Lets predict y value for the test dataset now\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "y_predict[:12], y_prob[:12]\n",
    "\n",
    " \n",
    "\n",
    "print(accuracy_score(y_test, y_predict))\n",
    "print(f1_score(y_test, y_predict, average='weighted'))\n",
    "\n",
    " \n",
    "\n",
    "#### Try SVM\n",
    "\n",
    " \n",
    "\n",
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1,random_state=1).fit(X_train_vectors_tfidf, y_train)\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1,random_state=1).fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    " \n",
    "\n",
    "To calculate the efficiency of the two models, we’ll test the two classifiers using the test data set:\n",
    "\n",
    " \n",
    "\n",
    "## Lets predict y value for the test dataset now\n",
    "poly_pred = poly.predict(X_test_vectors_tfidf)\n",
    "#poly_proba = poly.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "rbf_pred = rbf.predict(X_test_vectors_tfidf)\n",
    "#rbf_proba = rbf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    " \n",
    "\n",
    "Finally, we’ll calculate the accuracy and f1 scores for SVM with Polynomial kernel:\n",
    "\n",
    " \n",
    "\n",
    "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
    "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
    "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
    "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n",
    "\n",
    " \n",
    "\n",
    "print(classification_report(y_test, poly_pred))\n",
    "\n",
    " \n",
    "\n",
    "In the same way, the accuracy and f1 scores for SVM with RBF kernel:\n",
    "\n",
    " \n",
    "\n",
    "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
    "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')\n",
    "\n",
    " \n",
    "\n",
    "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
    "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
